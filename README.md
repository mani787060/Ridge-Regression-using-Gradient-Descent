# Ridge Regression Using Gradient Descent

-> This project implements **Ridge Regression (L2 Regularization)** completely from scratch using **Gradient Descent**.  
-> It demonstrates how regularization helps reduce overfitting by penalizing large weights and stabilizing the learning process.


## Project Highlights:-
-> Implemented Ridge Regression manually using Gradient Descent  
-> Mathematical formulation of cost function with L2 penalty  
-> Parameter updates derived step-by-step  
-> Comparison with **sklearn.linear_model.Ridge**  
-> Visualization of cost vs. iterations  
-> Shows effect of regularization strength (alpha)


## Contents:-
-> `ridge_gd_manual.ipynb` — Custom Ridge Regression using Gradient Descent  
-> `ridge_sklearn.ipynb` — Ridge Regression using sklearn  
-> Plots for cost minimization and coefficient shrinkage  
-> Sample dataset generation


## Concepts Covered:-
-> L2 regularization & weight shrinkage  
-> Gradient Descent update rule with penalty term  
-> Cost function behavior  
-> Bias–variance tradeoff  
-> Overfitting vs. underfitting curve


## Dependencies:-
-> Python  
-> NumPy  
-> Matplotlib  
-> Scikit-learn  


## Output:-
-> The model’s predictions, loss curve, and comparison with sklearn validate that the custom Ridge implementation matches expected results.

