{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b11a190-379f-4952-a49c-82bed0d5c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95612514-0650-4623-8ef7-acf0b9593fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39fa5b1-6978-468b-a7d7-a605be673658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call for train test split model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3147e92c-f6d8-4f50-92e1-6f3a6d4fbcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we look, how to apply \"gradient_descent\" wala \"Ridge_Regressor\" using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d6da8a-edee-44b0-a07f-f2cfa0353af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# obviously, it uses the \"Stochastic_Gradient_Descent\" wala version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd49e60-8580-4587-9f19-61584389b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SGDRegressor(penalty='l2', max_iter=500, eta0=0.1, learning_rate='constant', alpha=0.001)   # 'l2' --> L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840471cd-200c-47a1-8f8d-8cf03fab89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.45413504436575924\n"
     ]
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"R2 score\",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8804dac9-23ef-4da8-93bf-8eeb8924591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162.36789981]\n",
      "[  48.27996864 -155.72047654  370.24465796  269.78977572   -6.29083183\n",
      "  -59.5308553  -166.43271384  136.29915152  327.62328677   95.78601363]\n"
     ]
    }
   ],
   "source": [
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5adb0bb7-fcd8-4277-8f7e-f1e85b5f2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can solve it for 'Gradient_Descent' also by using 'Ridge_Regression'...\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg = Ridge(alpha=0.001, max_iter=500, solver='sparse_cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d588bb-3146-46c3-8c8e-75871a5f9647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_score: 0.46250101621736295\n"
     ]
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"R2_score:\", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66069148-af9e-44bb-8ba5-843f2c626008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.10198521698274\n",
      "[  34.52193767 -290.84083006  482.40183073  368.06788244 -852.44871299\n",
      "  501.59162206  180.1111415   270.76336403  759.73536616   37.49137216]\n"
     ]
    }
   ],
   "source": [
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae11048-8e77-4c0a-a027-ec56aaef485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to make our own class for same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0696e20-eea3-4e22-890c-2a4af7ab76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeraRidgeGD:\n",
    "\n",
    "    def __init__(self,epochs,learning_rate,alpha):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "\n",
    "        # Go to \"NOTES\" for better clarification :-\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "\n",
    "        self.intercept_ = 0 \n",
    "        self.coef_ = np.ones(X_train.shape[1])    # Add 1's to starting of the each column  \n",
    "\n",
    "        # we make a matrix named 'theta',in this total terms=(n+1), starting with 'intercept' and iske aage saare 'coefficients' honge.\n",
    "        # usme se 'n' terms --> coefficient and '1' term--> intercept term\n",
    "\n",
    "        theta = np.insert(self.coef_, 0, self.intercept_)   \n",
    "            \n",
    "             # it means we want to print our result in the form of [w0,w1,w2,....,wn] --> (which is the correct way)\n",
    "             # if we write, theta = np.insert(self.intercept_, 0, self.coef_),then it prints it in the form of [w1,w2,...,wn,w0] which is wrong way.\n",
    "\n",
    "             # Similarly, we also transform 'X_train' in similar way :- \n",
    "             \n",
    "        X_train = np.insert(X_train, 0, 1, axis=1)   # So, 'X_train' looks like this (with the extra 1's in the first column)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "             # Now, come for finding original_value of \"theta\" :- (use notes for this)          \n",
    "\n",
    "             theta_der = np.dot(X_train.T, X_train).dot(theta) - np.dot(X_train.T, y_train) + (self.alpha * theta)    \n",
    "             theta = theta - (self.learning_rate * theta_der)\n",
    "       \n",
    "              \n",
    "        self.intercept_ = theta[0]\n",
    "        self.coef_ = theta[1:]\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "\n",
    "        return np.dot(X_test, self.coef_) + self.intercept_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70d38bf4-74cd-4e4d-b2d7-b1b8ec68dde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_score: 0.47380182802609117\n"
     ]
    }
   ],
   "source": [
    "reg = MeraRidgeGD(epochs=500,alpha=0.001,learning_rate=0.005)\n",
    "\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"R2_score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# so, for same no. of epochs, it gives better \"r2_score\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5533c6a5-925d-43ea-946c-4f281e7ce7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.86975316713463\n",
      "[  46.65050914 -221.3750037   452.12080647  325.54248128  -29.09464178\n",
      "  -96.47517735 -190.90017011  146.32900372  400.80267299   95.09048094]\n"
     ]
    }
   ],
   "source": [
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (profiling310)",
   "language": "python",
   "name": "profiling310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
